# Getting data without an API - Web Scraping

Web scraping is the process of extracting information from a website, usually by parsing the html that is used by the web browser to display a page to the user.

This usually requires first manually inspecting the html in order to understand its structure, and how to access the parts you want.

A web crawler is a program that automatically follows links on a page to discover more content

![](images/wiki_table_scrape.png)

## Caveats

Read the terms of service:
 - many websites explicitely forbid web scraping
 - some websites (e.g.: Wikipedia) have a code of conduct for scraping

DO NOT scrape paywalled data, that's copyright infringement/piracy.

DO consider the research ethics of the data you're acquiring, especially if its about people.

Avoid placing undue strain on a website. Add reasonable delays between requests.

Be aware that you may get your IP blocked for unusual activity.

Scraping in and of itself is not illegal, but some of the applications of scraping may be.

Web scraping code is often fragile, slight updates to a websites design will require maintainance of the code.

## Crawling vs. scraping

Some ToS (e.g.: Reddit) make a distinction between crawling and scraping.

Crawling is the process of discovering and following links on pages.

Scraping is the process of extracting information from the contents of a page.

## Typical (research) uses

Retrieving tabular information.

Tracking changes in published information over time.

Comparing occurences of phrases, links or sentiments between groups of pages.

Building a map of links between pages (web spider/crawler)

## Basics
```python
import json
from bs4 import BeautifulSoup
import requests
import pandas as pd
import io

# Wikipedia countries list
url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'

# Identify ourselves as per T&Cs
headers = {'User-Agent': 'nelis.drost@auckland.ac.nz'}

# Get page
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

# Find the table containing the country names and links
table = soup.find('table', class_='wikitable')
rows = table.find_all('tr')

# Print the first few rows
df = pd.read_html(io.StringIO(str(table)))[0]
display(df.head())
```
